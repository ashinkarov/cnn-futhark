\section{Introduction\label{sec:intro}}

The year is 2025, and programmers still face a trade-off between correctness and
performance. Low-level languages like C or Fortran allow careful control over
hardware, but offer few guarantees about safety or program structure.
Dependently-typed languages like Agda or Lean, on the other hand, make it
possible to encode precise invariants, but typically lack the infrastructure for
high-performance computation.

Rather than chasing a mythical language that promises both complete correctness
and high performance, we explore a practical compromise: leveraging a
dependently-typed proof assistant for the parts of a scientific programming
pipeline where full verification is most crucial, and integrating it with a
high-performance backend for execution. Specifically, we investigate how Agda
can be used not only for specification and reasoning, but also for
transformation, optimisation, and code generation in a realistic machine
learning workload.

To explore this idea, we investigate the concrete problem of automatic
differentiation (AD) which is often found in machine learning applications. This
is a convenient case study as it comes with the following challenges. From the
correctness perspective, it is crucially important to track the shapes and ranks
of the tensors, guaranteeing the absence of out-of-bound indexing. This is a
very common source of errors that can be difficult to find. Secondly, we have to
compute derivatives of the given tensor expressions, preserve safe indexing
guarantees while we do so, and we have to be able to translate the computed
expressions into some high-performance language. As machine learning
applications are known to be numerically intensive problems, our performance
challenge lies in running the program as fast as we can on the chosen hardware
architecture.

In an ideal world, all parts would of course come with full formal
specifications and accompanying proof, but in practice some parts remain an
entirely separate and nontrivial research effort to verify for functional
correctness, such as AD. Other parts---such as code generation--can be handled
using known techniques, but are somewhat time consuming. Despite all this, we
show that even in those cases where resources preclude full verification, Agda
can still be used as a practical tool, with full verification employed where it
is deemed desirable and practical.

We follow~\cite{cnn-array} which demonstrates that it is possible to implement
one of the canonical convolutional neural network (CNN) in the array language
SaC~\cite{sac1, sac2}, obtaining good sequential and parallel performance that
is competitive with TensorFlow~\cite{ad-tf} and PyTorch~\cite{ad-pytorch}.
Focusing on correctness, we propose a theory of rank-polymorphic
arrays~\cite{rank-poly} in Agda~\cite{agda-2-6-3}. Within this framework, we
encode the CNN from~\cite{cnn-array} and lift it into an embedded DSL. We
implement AD (reverse mode) and domain-specific optimisations for expressions in
that DSL. Finally, we implement an extraction into Futhark (a functional array
language), apply the CNN to the MNIST digit recognition problem, and compare
performance with TensorFlow.

As a result, we demonstrate an approach where the entire specification,
optimiser, AD and code generation are available to us within a proof assistant
of our choice. We can prove facts about all the stages of the pipeline and
easily adjust them to our liking. We also demonstrate that some components can
be left only partially verified, where the effort of verification is judged
disproportionate to the benefit. We argue that such a liberating approach is
feasible in practice, at least for the times of dialectic of correctness and
performance. The overall goals is to demonstrate how to formulate a
computationally intensive problem in a proof assistant, transform it with
verification of key safety properties, and then pass it on to the other system
that can run the algorithm efficiently.

The contributions of this paper are as follows:
\begin{enumerate}
  \item a rank-polymorphic array theory and implementation of
        the CNN from~\cite{cnn-array} in Agda;
  \item a deeply-embedded DSL with HOAS wrappers in Agda which supports AD (reverse mode);
  \item an extraction mechanism for generating Futhark code from the DSL; and
  \item an experimental evaluation of the generated code.
\end{enumerate}

This paper is written in literate Agda, which guarantees that all the code
snippets have been type-checked.  All the code is available at \url{https://github.com/ashinkarov/cnn-futhark}.


