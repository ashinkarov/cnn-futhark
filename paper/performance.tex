\section{Performance\label{sec:performance}}

One of the goals of this work is to demonstrate that it is possible to
formulate the problem in a proof assistant and then pass it on to the
other system that can run the algorithm efficiently. In order to
substantiate this claim, we compare the running times of the code that
we generate from the specification of the CNN at the end of the
Section~\ref{sec:edsl} with an equivalent CNN implemented with
TensorFlow~\cite{ad-tf}. This is a limited study, and it does not
exploit all the expressivity provided by our language, but
nevertheless it shows the potential of achieving performance that
approaches that of established programs in non-verified languages. We
use the commonly used MNIST database, which consists of 60000
greyscale images of handwritten digits, each 28 by 28
pixels~\cite{deng2012mnist}.

Our verified CNN code corresponds only to a single forward and
backward pass. To build a full CNN, we implement by hand the
``batching loop'' that iterates across the training set, in which we
invoke the Futhark code extracted from the specification. Since the
specification is known to be free of indexing errors, we instruct the
Futhark compiler to elide bounds checking.

Our experimental results have been obtained on an NVIDIA A100 with
CUDA 11.8 and cuDNN 8.6.0, and are shown in
Table~\ref{tab:performance}. The runtime numbers exclude startup
costs, such as graph compilation, initialising GPU context, copying
data to the GPU, in order to focus on the performance of the generated
code. For the largest training set, these startup costs are about $5s$
seconds for TensorFlow and $6.7s$ for Futhark, which dwarfs the actual
training time, due to our small datasets.

While Futhark is competitive for the small workload, TensorFlow is
faster for the large ones. This is not surprising, as the individual
layers in the TensorFlow implementation are ultimately implemented
using hand-tuned primitives from cuDNN, which the Futhark compiler,
despite its optimisations, cannot match. Futhark's minor advantage on
the small workload is likely due to lower fixed overheads in the
generated code.

Profiling our generated code, we find that the Futhark compiler has
generated a rather large number of GPU kernels for the training loop,
most of which perform very little work, and contribute little to the
runtime. This is due to the normalisation issue discussed in
Section~\ref{sec:opt}, and shows that perhaps too many small arrays
are still manifested, that ought to be inlined. This is essentially
the common dilemma of how to balance recomputation with reuse.
However, the impact of these small GPU kernels on training performance
is minor--approximately 83\% of all GPU work occurs in two kernels
corresponding to the convolutional layers. These are also the layers
where cuDNN's implementation is significantly better than what the
Futhark compiler can produce.

\begin{table}
\begin{tabular}{crrr}
\textbf{Size of training set} & \multicolumn{2}{c}{\textbf{Runtime}} & \textbf{Ratio} \\
& Futhark & TensorFlow & \\
$10000$ & $0.91s$ & $1.07s$ & $0.85\times{}$ \\
$60000$ & $4.93s$ & $2.92s$ & $1.68\times{}$
\end{tabular}
\caption{Training time for various training set sizes, comparing
  Futhark and Tensorflow, running on an A100 GPU. The third column
  shows the speedup of TensorFlow compared to Futhark. We use a batch
  size of $1000$, a training rate of $0.05$, and train for $10$
  epochs.}
\label{tab:performance}
\end{table}

% Notes for Troels:
%
% - Leave Agda-side optimisations for Artem
% - Only backend: Futhark
% - Compared with:
%   - TensorFlow (on GPU)
%   - Hand-written Futhark (lower priority)
% - Will try for multicore numbers as well, if does not complicate story
