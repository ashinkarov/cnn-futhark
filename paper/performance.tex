\section{Performance\label{sec:performance}}

One of the goals of this work is to demonstrate that it is possible to
formulate the problem in a proof assistant and then pass it on to the
other system that can run the algorithm efficiently. In order to
substantiate this claim, we compare the running times of the code that
we generate from the specification of the CNN at the end of the
Section~\ref{sec:edsl} with an equivalent CNN implemented with
TensorFlow~\cite{ad-tf} and PyTorch~\cite{ad-pytorch}. This is a
limited study, and it does not exploit all the expressiveness provided
by our language, but nevertheless it shows the potential of achieving
performance that approaches that of established programs in
non-verified languages. We use the commonly used MNIST database, which
consists of 60000 greyscale images of handwritten digits, each 28 by
28 pixels~\cite{deng2012mnist}.

Our verified CNN code corresponds to a single training (forward and
backward) pass. To build a full CNN, we hand-implement the
``batching loop'' that iterates across the training set, in which we
invoke the Futhark code extracted from the specification. Since the
specification is known to be free of indexing errors, we instruct the
Futhark compiler to elide bounds checking.


Comparing performance of Futhark with Tensorflow and PyTorch is not
straight-forward.  Futhark is a classical ahead of time compiler,
whereas Tensorflow and PyTorch are large frameworks that depend on
many external libraries, therefore their performance is very
sensitive to system configuration.  Moreover, by default, Tensorflow
and PyTorch launch just-in-time compilers as a part of their runtime
which for our CNN takes about 5 seconds.
It is not clear whether there is an option to cache the result of
JIT compilation between the runs.  Here we pretend that this is the
case, and we exclude any identifiable startup costs and JIT time
from the measurements.

We run our experiments on an NVIDIA A100 with
CUDA 11.8 and cuDNN 8.6.0, and we summarise results for Futhark and
Tensorflow in Table~\ref{tab:performance}. 
While Futhark is competitive for the small workload, TensorFlow is
faster for the large ones. This is not surprising, as machine learning
frameworks are optimised for executing CNNs.  Individual
layers in the TensorFlow are ultimately implemented
using hand-tuned primitives from cuDNN, so it is very challenging
to outperform them from the general purpose Futhark compiler.
Futhark's advantage on the small workload is likely due to lower
fixed overheads in the generated code.

Our PyTorch version results in extremely high runtimes of 19 and 61
seconds, and we observe low GPU utilization. While this is likely to
be a problem with a system configuration, we report these numbers here
as an indication that performance of machine learning systems is
fragile.

Profiling our generated code, we find that 83\% of all GPU work occurs
in two kernels corresponding to the convolutional layers. These are also the layers
where cuDNN's implementation is significantly better than the one
that is generated by Futhark from a high-level specification.
For the rest 17\% of runtime, we observe that Futhark has
generated a rather large number of GPU kernels that perform relatively
little work.  Identifying a better strategy on reducing the number
of small kernels requires further research.  It might be beneficial to
inline some array computations to get rid of intermediate arrays;
or there might be a better way to split the computation into kernels.

Closing the observed performance gap is an interesting task.
While it is unlikely that Futhark can match cuDNN's performance
from the existing specification, our framework makes it possible
to adjust the generate code to help the compiler.  As long as cuDNN
does not use custom hardware for the entire kernel, an approach
based on high-level array blocking~\cite{rp-mm} might be applicable
in the context of GPUs.  We could clearly introduce specialised
primitives in the DSL, but it would result in a more
restricted language, which is less interesting from a research
perspective.

\begin{table}
\begin{tabular}{crrr}
\textbf{Size of training set} & \multicolumn{2}{c}{\textbf{Runtime}} & \textbf{Ratio} \\
& Futhark & TensorFlow & \\
$10000$ & $0.91s$ & $1.07s$ &  $0.85\times{}$ \\
$60000$ & $4.93s$ & $2.92s$ &  $1.68\times{}$
\end{tabular}
\caption{Training time for various training set sizes, comparing
  Futhark, Tensorflow, and PyTorch, running on an A100 GPU. The final column
  shows the speedup of TensorFlow compared to Futhark. We use a batch
  size of $1000$, a training rate of $0.05$, and train for $10$
  epochs.}
\label{tab:performance}
\end{table}

% Notes for Troels:
%
% - Leave Agda-side optimisations for Artem
% - Only backend: Futhark
% - Compared with:
%   - TensorFlow (on GPU)
%   - Hand-written Futhark (lower priority)
% - Will try for multicore numbers as well, if does not complicate story
