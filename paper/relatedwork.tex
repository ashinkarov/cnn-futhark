\section{Related Work\label{sec:relatedwork}}

In the following we relate the specifics of our contribution to prior
work. One important distinction however, which is easily obscured by
this detail-oriented comparison, is that our work combines parts that
have been subject to prior work, but are not commonly found together.

\paragraph{Automatic Differentiation}

Automatic differentiation has been around for many decades~\cite{early-ad1, early-ad2},
so it is well-understood at a conceptual level.  However,
a number of questions related to bringing AD into the context of
programming languages remain open.  Recent successes in machine learning
have spurred further interest in AD which has led to several new developments.
For the context of this paper, we focus on recent work that contributes to 
the perspective of balancing correctness guarantees and performance.
Our selection here is by no means exhaustive, for
a broader scope we refer the reader to~\cite{autodiff-survey}.

There has been a number of programming-language-oriented approaches that explain
how to add AD to a programming language of choice. Examples of these include
Futhark~\cite{futhark/sc22ad} and Haskell~\cite{ad-haskell}.
%
Furthermore, a number of machine learning
frameworks that incorporate AD have been proposed in recent years: TensorFlow~\cite{ad-tf},
PyTorch~\cite{ad-pytorch}, MXNet~\cite{ad-mxnet} and many more.
%
Most of these approaches are largely similar to ours (although naturally more
complete), as they involve a syntactically driven transformation of the program.

While in particular the dedicated frameworks tend to find widespread acceptance
by practitioners, both, correctness and performance leave two open questions:
(i) how is it possible to ensure that the AD algorithm is implemented correctly?
(ii) if the language does not perform as expected, what are the options to solve
this? Unfortunately, in many cases the answer to both questions is unsatisfying.
Most languages do not come with formal correctness guarantees, so one has to
trust the implementers of these tools. One can run tests as well to gain trust
in the implementation but that is far from a formal guarantee. If one relies on
the AD provided by a chosen language/framework, and the generated code does not
perform well, one has to modify the language/framework, as these solutions are
tightly integrated with the tools. The problem here is that most of of these
tools have very large and sophisticated implementations typically comprising of
hundreds of thousands of lines of code. Note that while our work does not give a
formal proof that AD implementation is correct, the entire implementation of AD
is about 20 lines of code, so it is easy to understand and modify. Formulating
correctness criteria for AD algorithms is somewhat involved, but has been done
for implementations where the AD algorithm monitors the program when it runs and
records a trace of its actions~\cite{de2023verifying}, while our approach is
based on static program transformation---this is in fact the only viable
approach when also doing program extraction.

The AD transformations in Dex~\cite{10.1145/3473593} and Jax~\cite{ad-jax} are
in this respect interesting, as they are based on first \emph{linearising} the
function of interest in a given point (essentially a forward mode
transformation), then \emph{transposing} the resulting linear map to obtain a
reverse mode linear map~\cite{radul2023you}. This approach might make be easier
to implement in a fully verified manner, as linearisation and transposition have
well-defined properties and could be verified independently.

Another line of work studies high-level principles of AD using category
theory~\cite{ad-theor1, ad-theor2, ad-theor3}. While this indeed comes with
great correctness guarantees due to some naturality principles, it is not always
clear how to implement this in a way that leads to efficiently executable
specifications, although some efforts are underway~\cite{10.1145/3632878}. Also,
the entire treatment of index-safe tensors is unclear.

In~\cite{ad-elliott} the author proposes to view AD problem using
the language of cartesian categories.  It has been shown that
this approach can be used in practice by implementing the proposed
technique in Haskell.  Type classes are a vehicle to restrict expressions
that are differentiable.  There is a Haskell plugin that translates
expressions that are instances of the mentioned type classes into
categorical primitives, AD is performed on these and the code is reflected
back to Haskell.  This is a nice approach that makes it easy
to verify the correctness of the algorithm.  However, the treatment
of tensors and general extractability remains a little unclear.
While it is briefly mentioned that representable functors
are supported, it is unclear whether this is sufficient to
represent rank-polymorphic arrays with strict bound checks.
Also, correctness guarantees are inevitably restricted by the
Haskell type system, so we are likely to find invariants that
are inexpressible in that setup.

\paragraph{Verified high performance computing}

One popular line of research is based on the idea of separating the
high level \emph{specification} of a problem from the \emph{schedule}
that describe how it is to be executed. Verifying that such schedules
are semantics-preserving is the topic of several recent
publications~\cite{10.1145/3527328,10.1145/3498717}. Our approach
differs by not using a scheduling language, and instead focuses on
verifying the specification itself, and also by having a particularly
expressive specification language.

Thiemann and Chakravarty demonstrated a prototype embedding of
Accelerate~\cite{10.1145/1926354.1926358} (a Haskell library for
accelerator programming) in Agda~\cite{thiemann2013agda}. Their
approach was based on dynamic code generation by invoking Accelerate
through Agda's foreign function interface, which posed various
unresolved implementation challenges. Even though our support for rank
polymorphism results in having even more complicated types, our code
extraction, which is essentially ahead-of-time compilation, is fairly
unproblematic.

Another related approach is the Etch compiler for indexed
streams~\cite{10.1145/3591268}, which is implemented as an embedded
language in Lean.  This work is very similar in spirit and it gives
a general encoding for sparse array computations.  The key difference
is that indexed streams fix traversal order of the index-space
of the given array, whereas \AD{Ar} only describes a mapping.
In fact indexed streams could be used to implement \AF{Ar}
computations.  It is a little unclear how to handle parallelism
in indexed streams.  \AF{Ar} expressions can always run parallel,
and this is one of the reasons why we can map them so easily to GPUs.


\paragraph{Type systems}

Verifying index operations is a classic application of dependent types, as seen
in for example Dependent ML~\cite{10.1145/292540.292560}, although most work
assumes static ranks. Our target language, Futhark, supports size-dependent
types~\cite{10.1145/3609024.3609412}, but this information is used only to
ensure shape conformance (e.g., that the operands to a matrix multiplication
have the same size), and does not guarantee that array indexing is in-bounds.
Single-Assignment C~\cite{sac2} (SaC) supports rank polymorphism, including
dynamic ranks, but does not guarantee that absence of indexing errors.

Gibbons showed how to express rank polymorphism in Haskell, through the use of
\emph{Naperian functors}~\cite{10.1145/2976022.2976023}. This form of rank
polymorphism is more limited than ours; in particular, it imposes static ranks.
This is also the case for libraries that provide rank polymorphic array
operations, such as Accelerate~\cite{10.1145/1926354.1926358} and
Repa~\cite{10.1145/1932681.1863582}. Further, Gibbons' work focused solely on
the embedding in Haskell, while our system also supports code extraction and
demonstrates good real-world performance. Similarly, Remora~\cite{rank-poly} is
a dependently typed rank-polymorphic language that also supports only static
ranks.
