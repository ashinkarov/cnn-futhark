> ICFP 2025 Paper #55 Reviews and Comments
> ===========================================================================
> Paper #55 Correctness Meets High Performance in Agda
> 
> 
> Review #55A
> ===========================================================================
> 
> Overall merit
> -------------
> C. Weak paper, though I will not fight strongly against it
> 
> Reviewer expertise
> ------------------
> 3. Knowledgeable
> 
> Reviewer confidence
> -------------------
> 3. High
> 
> Paper summary
> -------------
> This paper presents a methodology for embedding a DSL for rank-polymorphic multi-dimensional array computations in a dependently typed proof-assistant like Agda and extracting highly performant (Futhark) code from it. The DSL includes operations like multi-dimensional maps and zipWiths, indexing, and stencil operations. The authors demonstrate how the dependent types can be used to rule out errors like out-of-bounds indexing. The authors make their deeply embedded DSL usable through a HOAS-like wrapper based on Agda's instance arguments. They define an automatic differentiation code transformation on the DSL. The authors evaluate the performance of their generated code on a benchmark of a convolutional neural network written in the DSL.
> 
> Comments for authors
> --------------------
> The paper gives a nice showcase of how Agda's full dependent types can be used to embed DSLs for multi-dimensional array computations with guarantees on index-safety. While this presentation is quite nice, the idea of using dependent types for index-safety is an old one and goes back at least to 
> Xi, Hongwei, and Frank Pfenning. "Dependent types in practical programming." Proceedings of the 26th ACM SIGPLAN-SIGACT symposium on Principles of programming languages. 1999.
> Granted though, the application to multi-dimensional arrays is less common, though also done in e.g. 
> Paszke, Adam, et al. "Getting to the point: index sets and parallelism-preserving autodiff for pointful array programming." Proceedings of the ACM on Programming Languages 5.ICFP (2021): 1-29.
> The idea of showing how the type system is challenged/used when doing a type-safe AD transformation is also nice. The problem I see is that the AD transformation by itself does not seem interesting as there are no guarantees about either correctness (beyond out of bounds indexing) or complexity. The question then rises what the ultimate contribution of this paper is. It feels to me like the authors should try to articulate that more clearly.
> 
> 
> 
> Strengths:
> - Interesting new HOAS wrapper technique for deeply embedded DSLs based on Agda's instance arguments.
> - Nice case study of the use of full dependent types for index-safety in multi-dimensional arrays.
> 
> Weaknesses:
> - Only one (relatively simple) example program, a convolutional neural network.
> - It is unclear whether the code transformation that the authors implemented actually has the complexity expected of reverse AD and/or is efficient on any other programs than their one example. The authors say nothing about this. In fact, I strongly suspect it has the wrong complexity for a reverse AD technique and at least has logarithmic overhead due to the lack of in-place updates. This should at least be discussed in the paper.
> - The authors say nothing about the correctness of their AD technique and claim that "it is not clear how to formulate correcntess (sic) criteria for" AD. Over the last five years, there have been a large number of PL papers discussing correctness of AD, so I think that is a misunderstanding.
> 
> Questions:
> - How generally does your "unembedding" technique for HOAS-style wrappers of a deeply embedded DSL work? It works for your examples, but can you give any guarantees about the applicability of the technique? What are its limitations?
> - How does your "unembedding" technique relate to the technique from 
> Atkey, Robert, Sam Lindley, and Jeremy Yallop. "Unembedding domain-specific languages." Proceedings of the 2nd ACM SIGPLAN symposium on Haskell. 2009.
> ? Is there any reason not to use this tried and tested technique?
> - What is the complexity of the code generated by your AD transformation? Does it really deserve to be called reverse AD?
> - On l899 you say "direct compilation of the AD-generated expressions may be computationally inefficient". What are the main sources of inefficiency? Is there any way to classify them and address them systematically? Are your optimizations guaranteed to fix the problems on any other programs than your one example.
> - Do you have any reason to believe that your code transformation computes derivatives other than evaluation on your one example?
> 
> 
> Detailed comments for authors
> -----------------------------
> l5: "strong correctness guarantees" -- like/namely?
> 
> l80: "distinguishes it from most existing approaches" -- Sure, but there are also several rank polymorphic DSLs/libaries already, like Accelerate. Maybe mention them somewhere?
> 
> l102: say somewhere that S is simply a list of natural numbers
> 
> l102: It would be helpful to say here what the intended semantics of Ar [n1, ..., nk] is, i.e. R^{n1 x ... x nk} = R^{n1} (x) ... (x) R^{nk}.
> 
> l111: maybe note that you can define n-ary zipWith's even.
> 
> l119: corresponds
> 
> l136: why call this a sum rather than a fold? this is particularly confusing as later sum is just a sum.
> 
> l136: "pattern" -- quickly recall pattern syntax for readers less familiar with Agda
> 
> l160: "Dec" -- Just give the definition! It is not that hard and your current description is not enough to understand what it does.
> 
> l160: "\exists" -- Explain. Contrast with "\Sigma". Does the choice matter here?
> 
> l172-l180:  This is currently super unclear and should probably be rewritten.
> 
> l204: for
> 
> l204-205: It is unclear to me what the alternative is.
> 
> l220: ternary
> 
> l224: Explain instance argument syntax {{}} here.
> 
> l220-225: Say in prose what these relations do/capture.
> 
> l235: Remind readers of Agda's notation for identity types.
> 
> l258: Equations relating slide and backslide?
> 
> l297: "the local neighbourhood"
> 
> l298: rephrase sentence. grammar does not work.
> 
> l299: blocked selections "selb"
> 
> l329-342: very nice
> 
> l375: "use non-trivial dependencies within constructors" -- Please explain.
> 
> l401: "imaps" -- isn't this normally called build or generate. Why stray from that? 
> 
> l402:  "sels" -- isn't this just indexing? why not call it that?
> 
> l401-402: say in words what imap(s/b) and sel(s/b) are supposed to do.
> 
> l411: explain how zero-but gives a conditional.
> 
> l429: that we call imaps 
> 
> l433: equality.
> 
> l494: verb missing
> 
> l501-552: isn't this all standard? is the only point to introduce your notation?
> 
> l554-694: Interesting! I didn't know this technique. Is this a novel contribution? If so, maybe list it in the intro?
> 
> l702-735: Why so much white space? This seems like a perfect place to insert some diagrams/graphs!
> 
> l714-718: "inside-out....outside-in" -- This is quite vague and does not help the reader understand the difference between forward and reverse mode.
> 
> l716: reverse mode
> 
> l719: we can compute partial derivatives with respect to all of the inputs
> 
> l729: "computational graph" -- what is that? it comes out of nowhere here!
> 
> l786-791: please explain a bit more here.
> 
> l797-830: What is the complexity of the resulting algorithm? Is it as expected from reverse AD?
> 
> l925: is this how you fix expensive one-hot arrays?
> 
> l942-944: :-(
> 
> l1079: What about even larger datasets?
> 
> l1122: You may want to include some comparisons here to
> de Vilhena, Paulo Emílio, and François Pottier. "Verifying an Effect-Handler-Based Define-By-Run Reverse-Mode AD Library." Logical Methods in Computer Science 19 (2023).
> Paszke, Adam, et al. "Getting to the point: index sets and parallelism-preserving autodiff for pointful array programming." Proceedings of the ACM on Programming Languages 5.ICFP (2021): 1-29.
> Smeding, Tom J., and Matthijs IL Vákár. "Efficient CHAD." Proceedings of the ACM on Programming Languages 8.POPL (2024): 1060-1088.
> Smeding, Tom, and Matthijs Vákár. "Parallel dual-numbers reverse ad." arXiv preprint arXiv:2207.03418 (2022).
> 
> l1145: correctness 
> 
> l1171: "Even though our support for.."
> 
> l1185-1187: Explain more please. I don't understand the difference.
> 
> l1197: "certain functions being inverses" -- where do you use this?
> 
> 
> 
> Review #55B
> ===========================================================================
> 
> Overall merit
> -------------
> C. Weak paper, though I will not fight strongly against it
> 
> Reviewer expertise
> ------------------
> 2. Some familiarity
> 
> Reviewer confidence
> -------------------
> 2. Medium
> 
> Paper summary
> -------------
> The paper shows an approach to balance verified programming with high-performance, with the help of a dependently typed programming language generating high-performance machine learning code. The paper presents an Agda implementation of a rank-polymorphic array theory that is free of indexing errors, an embedded DSL for reverse-mode automatic differentiation, which can then be extracted to Futhark, a functional, array-oriented programming language designed for high-performance numerical computing on GPUs. A small experimental section compares the performance of the extracted code to TensorFlow on MNIST digit recognition problem.
> 
> ## Strengths
> 
> + Concise implementation of Array theory, CNNs and AD framework all presented together in Agda has great expository value. 
> + The paper is written in literate Agda; all code is type-checked. 
> 
> ## Weaknesses
> 
> - It is unclear what the novelty of the paper is. Many of the ideas that the paper presents are well known. The paper itself doesn't claim what the novelty is. A couple of important prior work, whose goals appear similar are missing. See 
>   - Paszke et al, "Getting to the Point: Index Sets and Parallelism-Preserving Autodiff for Pointful Array Programming", ICFP 2021. This paper appears to be solving the same problem, but with a novel programming language, whereas the current paper embeds it in Agda. 
>   - Wang et al, "Demystifying differentiable programming: shift/reset the penultimate backpropagator", ICFP 2019, https://dl.acm.org/doi/10.1145/3341700. Also aimed at bridging the gap between high-level programming and efficient low-level code, using metaprogramming. 
> - In the absence of novelty, the paper reads to me like a good functional pearl, condensing different ideas and presenting it in a unified framework.
> - Correctness guarantees are weak -- IIUC, the main correctness guarantee the paper ensures is indexing safety. There are no correctness guarantees for AD (and the authors point out that it is unclear how to specify this), nor are there any for the optimisations done in sections 5.2 and 5.3. Perhaps the optimisation can be shown correct with respect to the unoptimised version? 
> - Weak evaluation -- the only benchmark that's there is the MNIST training. While the performance is on par with TensorFlow, it is hard to draw conclusions from a single benchmark. PyTorch results seems quite off, and further work is necessary to understand what the issue is and redo the experiments.
> 
> Comments for authors
> --------------------
> ## Comments
> 
> * Line 76, "The work in the rest of the paper is presented in Agda, with which we assume some familiarity." I appreciated the comment upfront. The paper assumes a fair bit of Agda knowledge even for someone who has used other proof assistants and dependently typed programming languages.
> * Line 162, "Recall that the type Fin n is a type for natural numbers i that are bounded by n (i.e. i < n)." This should come at first use. See Line 104.
> * Line 1145, "implementatuion" implementation.
> 
> 
> 
> Review #55C
> ===========================================================================
> 
> Overall merit
> -------------
> B. OK paper, but I will not champion it
> 
> Reviewer expertise
> ------------------
> 2. Some familiarity
> 
> Reviewer confidence
> -------------------
> 2. Medium
> 
> Paper summary
> -------------
> The authors give an in depth case study to examine the following thesis:
> 
> > It is possible to write high-performance code in a dependently-typed programming language without
>   sacrificing either speed or correctness guarantees.
> 
> In particular, they construct a DSL in Agda which is automatically free of various errors and allows
> them to write a CNN implementation which is subsequently compiled to a low-level language for
> efficient execution.
> 
> Over all, I enjoyed reading this paper and found the actual Agda code to be surprisingly readable
> and high-level, even for someone with limited experience in ML algorithms. I have included two
> questions for the rebuttal below along with several specific smaller comments.
> 
> Comments for authors
> --------------------
> # Question for the rebuttal
> 
>  - I'm having a hard time interpreting the benchmark at the end of the day, which compares the
>    generated code to a corresponding algorithm in Python. The fact that the overhead for the initial
>    load for the Python benchmark is nearly >3x the length of the actual computation also suggests
>    that this is too noisy to be a meaningful comparison. Would it be possible to either (1) increase
>    the training sizes to give a better sense of how this program performs on larger cases (2)
>    compare it to another compiled language so as to avoid making arbitrary decisions on accounting
>    for JIT behavior and perhaps even (3) include a non-DSL-compiled version of the algorithm to get
>    sense of how much the extraction to Futhark and compilation helps over a more naïve approach?
> 
>    I'm far from an expert on benchmarking GPU programs, so perhaps the authors' can help clarify why
>    the results in 6 are really significant and indicate much about the actual relative performances
>    of the programs being considered.
> 
>  - Are there any correctness guarantees for the compiler to Futhark or an indication of what those
>    correctness theorems should be? In particular, how confident can I be that the behavior of my
>    program matches what I observe in Agda/that my theorems about what it computes will transfer?
> 
> # Various Small Comments
> 
>  - 52: "focussing" should be focusing
>  - 76: minor stylistic comment: the superscript 1 should be on the period, not the last word in the
>    sentence.
>  - 86: I'm a little confused, should an array of rank 0 not be simply the trivial vector
>    space/module? The space of scalars is dimension 1, not 0 in standard mathematics in my
>    experience.
>  - 101: I can see why given the code things work out as they do, but this seems like an artifact of
>    how the indexing is set up: the smallest representable array is 1 by 1, not 0 by 0... Presumably
>    this is fine, because we do not need something like this, but is this really the standard
>    indexing convention in machine learning?
>  - 112: As a small side, zipWith should come from a more primitive operation which witnesses
>    `Ar s (X × Y) ≅ Ar s X × Arr s Y` (so this is pointed cartesian functor), but this is neither
>    this is a matter of aesthetics :)
>  - 130: this is another "X is just Y" of the highest order, but I suppose one can remark that `nest`
>    and `unnest` ensure that `Ar - X` is a monoidal functor from `Shape` to `Type → Type` (with the
>    latter monoidal product coming from composition). Hardly the a pressing issue, but I found it
>    neat.
>  - 136: I think one should briefly explain what "pattern" does in Agda, since this sort of syntax is
>    a bit more Agda-specific than what has come before. I think it can be guessed from context, but a
>    footnote or just a description of what to search in the Agda docs might be helpful.
>  - 218: "that 𝑞 is a point-wise addition of 𝑝 and 𝑞" should be r.
>  - 228: Again, this footnote should be on the period.
>  - 221: Incidentally, why is this specialized to natural numbers? Surely this works fine for an
>    arbitrary type A.
>  - 465: I'm surprised that passing the environment as an instance argument is a safe idea here:
>    could Agda not always update in the future to make this code just pass the empty environment
>    instead?
>  - 716: "revers" to "reverse"
>  - 720: What is a "computational graph" in this context? Is it the dependency graph of expressions?
>  - 725: 𝑤2= 𝑤1𝑤2 has a typo
>  - Is the terminology adjoint standard? I've seen "partials" or "1-forms" for these objects.
> 
> 
> 
> Review #55D
> ===========================================================================
> 
> Overall merit
> -------------
> A. Good paper, I will champion it
> 
> Reviewer expertise
> ------------------
> 3. Knowledgeable
> 
> Reviewer confidence
> -------------------
> 2. Medium
> 
> Paper summary
> -------------
> This paper does not present anything revolutionary new, but pieces together existing components, such as statically typed tensors, EDSLs in dependently-typed languages, efficient functional array languages to a meaningful whole:  An EDSL embedded in Agda that let's us extract efficient Futhark programs for tensor manipulation.
> 
> I think it makes a good and timely ICFP contribution, however, the quality of paper and artifact need to see improvements before publication.
> 
> Strong points:
> 
> + Good motivation: use Agda with all of its type safety as a modelling language for machine learning algorihms, have an efficient backend (Futhark) to run the algorithms.
> + Elegant representation of tensors as containers `List ℕ ▷ All Fin` and subsequent nice definitions of convolution neural networks.
> + A statically well-typed (de Bruijn) DSL of arrays and indices with a surface layer of HOAS binders
> + Automatic differentiation implemented as symbolic differentiation on the DSL experssions
> + Extraction to Futhark via some normalization by evaluation algorithm, yielding well-performing results.
> 
> Weak points:
> 
> - Differentiation (Section 5) is badly explained.
> - No graphical illustrations that would help understanding the array manipulations.
> - No abstract syntax of Futhark used in extraction, makes it hard to verify the extraction.
> - The artifact is not very polished, could not get everything to run even with substantial investment.
> 
> Comments for authors
> --------------------
> - l50ff "follow [36]" here and everywhere: do not use citations as nouns

Sure, happy to fix this.
> 
> - l99 Maybe add a footnote to the extend that `S ▷ P` is a container (I guess that motivated the letters `S` and `P`).
>   However, I agree with your choice to not overload the paper with technical terms that do not help understanding.

Happy to add a footnote.

> - l103 It is worth remarking that an array of dimension 0 (shape `[]`) is a singleton ("skalar").

We actually say this in line 86/87: "The `[]` shape describes an array of rank zero that contains exactly one element"

> - l136 Is there a reason to use `ι n` instead of the standard `[ n ]` for the singleton `n`?

Not really, so one could use `[ n ]` instead.

> - l137ff What is called `sum` is really a `fold`, even if you only use it with addition later.

Kind of, except it doesn't fold the unrolling of the array, it folds a hyperplane ar a time, so I am not sure whether the name fold helps here.

> - l142 Why is the base case not simply `sum {s = []} f ε a = a []`?
>   I tried this out with your Agda artifact and it works, one can simplify some proofs.
>   An array of zero dimensions is a single scalar.  A combination of all its elements is thus just this one scalar.
>   I think your starting point was the `foldr` intuition where the result always in corporates the neutral element,
>   but maybe thinking in terms of non-empty folds (`fold1`) is more appropriate here.

Exactly as you say, if `ε` is neutral to `f` then it doesn't matter.
However, with lists, if you fold a singleteon list `fold f e [ x ]` you end up with `f x e` rather than `e`.  As arrays of shape `[]` are singletons, I thought it is consistent to do the application of `f`.

> - l169 I think the surprise goes away if the second hypothesis is written as $j ≤ n$ rather than $j + 1 < n$.

My line of thought was: `i : Fin m` translates to `i < m` and `j : Fin (1 + n)` translates to `j < 1 + n` (as I explained in line 163).  Then, for premises `a < b` and `c < d` it feels natural to conclude `a + c < b + d`, which is true, but it is also an overaproximation.  I am happy to add the comment saying that `j < 1 + n` is the same as `j ≤ n`.


> - l205 "fot" → for 
> - l216 "that $q$ is a point-wise addition" → that $r$ is ...
> - l220 "trenary" → ternary
> - l232 "f (g x)" and "f (g x y)" are in the wrong font
> - l246 "The implementation[s] of..."  Grammar and missing full stop.

Sure, thanks for spotting those.

> - l295ff A picture would help in the visualization of block formation (for low dimensions).

We are a bit pressed on space, but I can try to add it.

> - l299 "introducing blocked selections" insert `selp` here

You mean `selb`, sure.

> - l309 in the type you could have `Ar p (Ar s X)` instead of `P p → Ar s X` which would make it more symmetrical with `imapb`

Sure, thanks.

> - l419 why does `scaledown` take an `ℕ` and not an `ℝ`?

For simplicity really.  The definition of `E` keeps the type of reals completely opaque, e.g. it is a base type of the arrays of these shapes which you chose when you interpret the language.  Any real type you chose should allow conversions from natural numbers.  I could parametrise E with `ℝ`, or
use `E Γ unit` as an argument, natural number seemed to be the simplest option.

> - l456 "expressions in E Γ _is_" use parentheses "(E Γ _is_)"

Sure, thanks.

> - l696ff This section may offer opportunities for salvaging some space if you are struggling with the page limit:
>   it feels things are spaced out much more generously than in the other sections.

Yes, you are right, we can compress the list of equations.

> - Section 5: There is a drop in quality in the exposition of this section.  It deserves some serious polishing.

We'd be more than happy to try our best to improve this section.

> - l716 "revers[e] mode"

Sure.

> - l717 Having to juggle with the aliases $y = w₃$ and $x = w₀$ obstructs the pattern.

Fair enough.

> - l720 What role is served by $f x y$ here?  Why not simply $z = \sin(xy + x)$.

Agreed.

> - l720 $sin$ should be $\sin$.

Sure, thanks. 

> - l724 $w₂ = w₁ w₂$ I guess this should be $w₂ = w₀ w₁$.  The aliases even confuse you.
> - l729 ∂y should be ∂z, should it?

Yes to both, and thanks for noticing.
> 
> - l729 Please state what a "successor in the computational graph is", by example.
>        E.g. for $w₂$ is $w₃$ the successor or are these $w₀$ and $w₁$?
>        Make sure to include an example with more than one successor!
> 
> - l737ff I am totally lost here.  Please provide more calculation steps for each line.
>          You got plenty of unused horizontal space here!
> 
> - l746 "If we inline all the $\bar{w}ᵢ$ definitions"  Again more steps please.
>     If I do this myself, I am left with $\bar{w}₀ = \cos w₃ + (\cos w₃)²·x$ which isn't close to the results you report.

I will redo the example providing more intermediate steps.

> - l1000 This parenthesis never closes.

This is a typo, thanks.

> - l1015 So Futhark code is represented just by a `String`?
>         Why don't you model the relevant parts by some abstract syntax?
>         Can you even prove something about your normalization function when you juggle with monsters like `String → String` to represent a Futhark context? 


Yes, you are right, it would be nicer to prove safe translation to Futhark by
defining a model of Futhark, and then relating the interpretation of futhark programs
and evaluation of E.  This does require more work though, and in practice,
code generation seemd to be the least interesting part of the work.
We thought that the proposed way might be a reasonable compromise that gets
us NBE without introducing much scaffolding.

> 
> 
> 
> Artifact: agda
> ==============
> 
> Ar.agda
> -------
> 
> I suppose some functions you define here can also be found in the standard library, e.g.
> 
> - `_++_ = Data.List.Relation.Unary.All.Properties.++⁺`
> - `_∙_ = trans`
> - `inject-left = _↑ˡ_`
> - ...

Sure, I recall that we had troubles with different versions of the standard library, so we just replicated some of the functions.  We can clean fix the library version and do a little cleanup.

> 
> 
> Artifact: src
> =============
> 
> This artifact does not seem to be very portable, and the README does not list the system requirements.
> I tried to run the experiment on a recent Mac.
> 
> - The python `tensorflow` library does not seem to support Python 3.13, so I had to downgrade Python to 3.12,
>   and then remove the `tensorflow` bound in the `requirements.txt` file.
> - This enabled me to run the Python experiments in the `pytorch` and `tensorflow` directories.
> - I could not get the futhark experiment to run, as the invocation of `futhark cuda` failed.
> 
> The script contains very non-portable code.
> ```
> export LD_LIBRARY_PATH=/opt/software/cudnn/8.6.0/lib/:$LD_LIBRARY_PATH # Hack
> ```
> Also `module` is not preinstalled on macOS and even after `brew install modules` I did not get it to work from inside the script.
> ```
> $ ./experiment.sh
> ./experiment.sh: line 12: module: command not found
> 
> $  module --version
> Modules Release 5.5.0 (2024-11-11)
> ```
